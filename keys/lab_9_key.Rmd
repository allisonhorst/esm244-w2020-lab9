---
title: "Lab 9: Bootstrapping, nonlinear least squares, and customizing tables with gt"
author: "Allison Horst"
date: "3/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)
library(boot)
library(gt)
library(patchwork)
library(broom)
library(nlstools)

# Note: get gt package from GitHub
# library(devtools)
# remotes::install_github("rstudio/gt")
```

### Fun, beautiful tables with `gt`

We'll use the `LifeCycleSavings` built-in dataset. See `?LifeCycleSavings` for more information. 

Simplify it a bit to get the 10 countries with the lowest savings ratio:
```{r}
disp_income <- LifeCycleSavings %>% 
  rownames_to_column() %>% 
  arrange(dpi) %>% 
  head(5) %>% 
  mutate(ddpi = ddpi / 100,
         pop15 = pop15 / 100,
         pop75 = pop75 / 100) # To make it a decimal (we'll convert to a percentage in the table - better practice)
```

`gt` is an awesome package that makes customizing tables relatively painless. See a bunch of options here: 
https://gt.rstudio.com/reference/tab_options.html

We'll make a custom table using the `gt` package with the following goals: 

- Percent variables (ddpi, pop15 and pop75) should be in percent format
- Per capita disposable income (dpi) should be as dollars
- Color of dpi cells should change based on value

Build this one level at a time to see changes: 
```{r}
disp_income %>% 
  gt() %>% 
  tab_header(
    title = "Life cycle savings", # Add a title
    subtitle = "5 countries with lowest per capita disposable income" # And a subtitle
  ) %>% 
  fmt_currency( # Reformat to currency notation...
    columns = vars(dpi), # The values for the 'Height' variable
    decimals = 2 # Keeping 3 decimal places
  ) %>% 
  fmt_percent(
    columns = vars(pop15, pop75, ddpi),
    decimals = 1
  ) %>% 
  fmt_number(
    columns = vars(sr),
    decimals = 1
  ) %>% 
  tab_options(
    table.width = pct(80)
  ) %>% 
  tab_footnote(
    footnote = "Data averaged from 1970 - 1980",
    location = cells_title()
  ) %>% 
  data_color( # Update cell colors...
    columns = vars(dpi), # ...for mean_len column
    colors = scales::col_numeric(
      palette = c(
        "orange", "red", "purple"), # Overboard colors! 
      domain = c(120,190) # Scale endpoints (outside will be gray)
  )
  ) %>% 
  cols_label(
    sr = "Savings ratio",
    pop15 = "Pop < 15yr",
    pop75 = "Pop < 75yr",
    dpi = "Disposable $ per capita",
    ddpi = "Disposable percent"
  )
```

### Bootstrapping!

Bootstrapping uses **sampling with replacement** to find a sampling distribution that is based on more than a single sample. Let's bootstrap a 95% confidence interval for the mean salinity of river discharge in Pamlico Sound, NC (see `?salinity` for information on the dataset from the `boot`) package. 

From dataset documentation: "Biweekly averages of the water salinity and river discharge in Pamlico Sound, North Carolina were recorded between the years 1972 and 1977. The data in this set consists only of those measurements in March, April and May."


Look & explore `salinity` dataset. 
```{r}
# View(salinity)

# Get some summary statistics from the single salinity sample:
hist(salinity$sal)
mean(salinity$sal)
t.test(salinity$sal) # Get 95% CI for t-distribution

```

ALWAYS ask questions: 

- Do the data look normally distributed? 
- Do we have a large enough n to depend on Central Limit Theorem? 
- What assumptions do we make if we find the CI based on a single sample using the t-distribution here? 

Bootstrap the mean salinity: 
```{r}

# First, create a function that will calculate the median of each bootstrapped sample
mean_fun <- function (x,i) {mean(x[i])}

# Then, get just the vector of salinity (salinity$sal)
sal_nc <- salinity$sal

# Now, create 100 bootstrap samples by resampling from the salinity vector (sal_nc), using the function you created (mean_fun) to calculate the mean of each:
salboot_100 <- boot(sal_nc, 
                    statistic = mean_fun,
                    R = 100)

# OK, then for comparison, let's also create 10000 bootstrap samples with replacement
salboot_10k <- boot(sal_nc, 
                 statistic = mean_fun, 
                 R = 10000)

# Check out the output from the bootstrap:
salboot_100
salboot_10k

# Question: do we all get the same thing? What would we have to do in order to all get the same thing (hint: set.seed())? And would we *want* that? 

# Use $t0 to see the original sample mean, and $t to see the means for each of the bootstrap samples:
salboot_100$t0 # The original sample mean
salboot_100$t # These are all the means for each 100 samples

# Make vectors of bootstrap sample means a data frame (so ggplot will deal with it). 
salboot_100_df <- data.frame(bs_mean = salboot_100$t)
salboot_10k_df <- data.frame(bs_mean = salboot_10k$t)

# ggplot the bootstrap sample medians: 

# The histogram of the original sample:
p1 <- ggplot(data = salinity, aes(x = sal)) +
  geom_histogram()

# Histogram of 100 bootstrap sample means:
p2 <- ggplot(data = salboot_100_df, aes(x = bs_mean)) +
  geom_histogram()

# Histogram of 10k bootstrap sample means:
p3 <- ggplot(data = salboot_10k_df, aes(x = bs_mean)) +
  geom_histogram()

# Aside: PATCHWORK IS AWESOME!!! 
p1 + p2 + p3
```

Seriously, check out how awesome `patchwork` by Thomas Lin Pedersen is: https://patchwork.data-imaginist.com/articles/guides/assembly.html...a game changer for piecing together compound figures! 

It also knows PEMDAS
```{r}
p1 + p2 / p3
```

```{r}
(p1 + p2) / p3
```

## OK back to bootstrapping...

So now we have a sampling distribution based on means calculated from a large number of bootstrap samples, and we can use *this* sampling distribution (instead of one based on assumptions for our single sample) to find the confidence interval. 

```{r}
boot.ci(salboot_10k, conf = 0.95)
```

We'll chat a bit about what each of these CIs means, and you can read more here: https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2656.12382

A reminder on what a confidence interval means: For a 95% confidence interval, that means we expect that 95 of 100 calculated confidence intervals will contain the actual population parameter. 

What it does **not** mean: There's a 95% chance that your confidence interval contains the true population parameter (it either does or it doesn't)...

And you can bootstrap all kinds of other things too (medians, variances, correlations, regression coefficients, etc.), but there are limitations.

### Nonlinear least squares

Read in some mock logistic growth data:
```{r}
df <- read_csv(here("data", "log_growth.csv"))

# Look at it:
ggplot(data = df, aes(x = time, y = pop)) +
  geom_point() +
  theme_minimal() +
  labs(x = "time (hr)", y = "population (ind)")

# Look at the log transformed:
ggplot(data = df, aes(x = time, y = log(pop))) +
  geom_point() +
  theme_minimal() +
  labs(x = "time (hr)", y = "ln(population)")
```

Recall: 

$P(t)=\frac{K}{1+Ae^{-kt}}$, where

- $K$ is the carrying capacity
- $A$ is $\frac{K-P_0}{P_0}$
- $k$ is the growth rate constant

### Initial estimates for *K*, *A* and *k*

Estimate the growth constant during exponential phase (to get a starting-point guess for *k*): 
```{r}
# Get only up to 14 hours & ln transform pop
df_exp <- df %>% 
  filter(time < 15) %>% 
  mutate(ln_pop = log(pop))
  
# Model linear to get *k* estimate:
lm_k <- lm(ln_pop ~ time, data = df_exp)
lm_k

# Coefficient (k) ~ 0.17
```

Now we have initial estimate for *k* (0.17), and we can estimate *K* ~180 and *A* ~ 17. We need those estimates because we will use them as starting points for interative algorithms trying to converge on the parameters. If we're too far off, they may not converge or could converge on the very wrong thing.

Two methods we'll try to estimate the parameters: nonlinear least squares (NLS) and Maximum Likelihood Estimation (MLE). 

### Nonlinear least squares

Nonlinear least squares converges on parameter estimates that minimize the the sum of squares of residuals through an iterative algorithm (we'll use Gauss-Newton, the most common). 

Check out the `stats::nls()` function (`?nls`) for more information. 

Now enter our model information, with a list of estimated starting parameter values:
```{r}
df_nls <- nls(pop ~ K/(1 + A*exp(-r*time)),
              data = df,
              start = list(K = 180, A = 17, r = 0.17),
              trace = TRUE
              )

# Note: you can add argument `trace = TRUE` to see the different estimates at each iteration (and the left-most column reported tells you SSE)

summary(df_nls)

# Use broom:: functions to get model outputs in tidier format: 
model_out <- broom::tidy(df_nls)

# Want to just get one of these? 
A_est <- tidy(df_nls)$estimate[1]

```

Our model with estimated parameters is:
$$P(t) = \frac{188.7}{1+138.86e^{-0.35t}}$$

### Visualize model
```{r}
t_seq <- seq(from = 0, to = 35, length = 200)

# Make predictions for the population at all of those times (t)
p_predict <- predict(df_nls, newdata = t_seq)

# Bind predictions to original data frame:
df_complete <- data.frame(df, p_predict)

# Plot them all together:
ggplot(data = df_complete, aes(x = time, y = pop)) +
  geom_point() +
  geom_line(aes(x = time, y = p_predict)) +
  theme_minimal()

```

### Find confidence intervals for parameter estimates

See `?confint2` and `?confint.nls`
```{r}
df_ci <- confint2(df_nls)
df_ci
```

### END LAB 9
